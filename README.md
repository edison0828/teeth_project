# Oral X-Ray Intelligence Platform

## ğŸŒ Language / èªè¨€

- [English](#english)
- [ä¸­æ–‡](#ä¸­æ–‡)

---

## English

### 1. Overview

The Oral X-Ray Intelligence Platform combines a FastAPI backend, a Next.js 14 (App Router) frontend, and legacy PyTorch training utilities to streamline oral radiograph analysis. It offers interactive dashboards, simulated AI-assisted workflows, and clear extension points for future integrations.

### 2. Demo & Screenshots

<!-- æ·»åŠ æ‚¨çš„ç¶²é æˆªåœ– -->

![Cross-Attention Analysis](docs/images/cross-attention-demo.png)
_Cross-attention Grad-CAM visualization for dental condition detection_

**ğŸ“º Demo Video**: [Watch the demo demonstration](https://youtu.be/ONw7fAoHVxE)

### 3. Repository Layout

```
teeth_project/
â”œâ”€â”€ backend/                  # FastAPI application with sample in-memory data
â”‚   â”œâ”€â”€ main.py                # REST endpoints and orchestration helpers
â”‚   â”œâ”€â”€ schemas.py             # Pydantic domain models
â”‚   â””â”€â”€ requirements.txt       # Python dependencies for the API
â”œâ”€â”€ frontend/                 # Next.js 14 (App Router) user interface
â”‚   â”œâ”€â”€ app/                   # Dashboard, Patients, Upload, and Analysis Result pages
â”‚   â”œâ”€â”€ components/            # Shared navigation, cards, widgets
â”‚   â”œâ”€â”€ lib/                   # Typed API client utilities
â”‚   â””â”€â”€ package.json           # Frontend dependencies and scripts
â”œâ”€â”€ docs/                     # System design documentation
â”œâ”€â”€ models/                   # Saved model artifacts or checkpoints
â”œâ”€â”€ src/                      # Legacy PyTorch training and inference scripts
â””â”€â”€ data/                     # Sample datasets and supporting files
```

### 3. Prerequisites

| Component | Requirement                                         |
| --------- | --------------------------------------------------- |
| Backend   | Python 3.10+ and `pip`                              |
| Frontend  | Node.js 18+ and `npm` or `yarn`                     |
| Optional  | `make` or shell access for running combined scripts |

> **Tip:** Use a Python virtual environment to isolate dependencies and avoid conflicts with other projects.

### 4. Quick Start

1. **Clone the repository** (or open the project folder if already available).
2. **Create and activate a Python virtual environment:**
   ```bash
   python -m venv .venv
   source .venv/bin/activate  # Windows: .venv\Scripts\activate
   ```
3. **Install backend dependencies and start the API:**
   ```bash
   pip install -r backend/requirements.txt
   uvicorn backend.main:app --reload
   ```
   The API defaults to `http://localhost:8000` and serves mock data for immediate testing.
4. **Install frontend dependencies and start the web UI:**
   ```bash
   cd frontend
   npm install
   npm run dev
   ```
   Access the UI at `http://localhost:3000`. Ensure the backend remains running in a separate terminal.

### 5. Configuring the Frontend

Set the `NEXT_PUBLIC_API_BASE_URL` environment variable if the backend is hosted somewhere other than `http://localhost:8000`:

```bash
NEXT_PUBLIC_API_BASE_URL="https://your-api.example.com" npm run dev
```

### 6. Sample Workflow

1. Log in to the dashboard (no credentials required for the mock implementation).
2. Review the **Dashboard** page for system metrics, analysis queue status, and quick actions.
3. Open **Patients** to browse and search the mock patient registry. Select a patient to see demographics and history.
4. Navigate to **Upload** to drag-and-drop an oral radiograph image (placeholders are accepted). Provide patient/study details and submit to trigger the simulated pipeline.
5. View the generated report on the **Analysis Result** page, which displays detected conditions, AI findings, and a processing timeline.

### 7. API Reference (Development Mode)

| Endpoint                  | Method     | Description                                   |
| ------------------------- | ---------- | --------------------------------------------- |
| `/api/dashboard/overview` | GET        | System summary, queue status, recent patients |
| `/api/patients`           | GET / POST | List existing patients or create new entries  |
| `/api/analyses/{id}`      | GET        | Retrieve findings for a specific analysis     |
| `/api/images`             | POST       | Submit an image payload for processing        |
| `/api/analyses`           | POST       | Kick off a simulated analysis job             |

Responses return curated sample payloads that match the design specification, making it easy to develop the UI without real data sources.

### 8. Legacy Machine Learning Utilities

The original PyTorch scripts located in `src/` remain intact for researchers who need to train or evaluate models. Each script contains inline comments describing expected CSV/DICOM formats and configuration options.

### 10. Cross-Attention Demo

A lightweight FastAPI + Next.js experience is available to showcase the cross-attention Grad-CAM inference pipeline.

![Demo Interface](docs/images/demo-interface.png)
_Cross-attention demo interface with sample cases and upload functionality_

**ğŸ¬ Cross-Attention Demo Video**: [See the AI analysis in action](https://youtu.be/ONw7fAoHVxE)

- **Backend**: `demo_backend/main.py` exposes `/demo/samples` and `/demo/infer`. Configure weights via environment variables such as `DEMO_YOLO_WEIGHTS` and `DEMO_CLASSIFIER_WEIGHTS`.
- **Frontend**: visit `/demo` in the Next.js application to browse bundled cases or upload an image.
- **Static assets**: curated demo samples live in `demo_backend/static/samples/`; simply drop PNG/JPG files and the API will auto-discover them.
- **Outputs**: inference artifacts are stored under `demo_backend/outputs/` and surfaced through `/demo-outputs`.

### 9. Next Steps & Customization Ideas

- Replace in-memory stores with persistent databases and object storage.
- Connect the upload form to real file-handling services or presigned URLs.
- Integrate authentication/authorization (e.g., OAuth2 with RBAC) according to the design documentation.
- Extend the analytics pipeline to incorporate real inference results from the `models/` directory.

---

## ä¸­æ–‡

### 1. å°ˆæ¡ˆç°¡ä»‹

å£è…” X å…‰æ™ºæ…§å¹³å°çµåˆ FastAPI å¾Œç«¯ã€Next.js 14ï¼ˆApp Routerï¼‰å‰ç«¯ä»¥åŠæ—¢æœ‰çš„ PyTorch è¨“ç·´å·¥å…·ï¼Œæä¾›å®Œæ•´çš„å£è…”æ”¾å°„å½±åƒåˆ†æé«”é©—ã€‚ç³»çµ±å…§å»ºäº’å‹•å¼å„€è¡¨æ¿ã€æ¨¡æ“¬ AI å·¥ä½œæµç¨‹ï¼Œä¸¦ä¿ç•™å‘çœŸå¯¦æœå‹™æ•´åˆçš„æ“´å……å½ˆæ€§ã€‚

### 2. ç¤ºç¯„èˆ‡æˆªåœ–

<!-- æ·»åŠ æ‚¨çš„ç¶²é æˆªåœ– -->

![Cross-Attention åˆ†æ](docs/images/cross-attention-demo.png)
_Cross-attention Grad-CAM è¦–è¦ºåŒ–ç”¨æ–¼ç‰™ç§‘ç–¾ç—…æª¢æ¸¬_

**ğŸ“º ç¤ºç¯„å½±ç‰‡**: [è§€çœ‹å®Œæ•´å¹³å°æ¼”ç¤º](https://youtu.be/ONw7fAoHVxE)

### 3. å°ˆæ¡ˆçµæ§‹

```
teeth_project/
â”œâ”€â”€ backend/                  # FastAPI æ‡‰ç”¨ç¨‹å¼ï¼Œæä¾›å…§å»ºæ¸¬è©¦è³‡æ–™
â”‚   â”œâ”€â”€ main.py                # REST API èˆ‡æµç¨‹å”èª¿å·¥å…·
â”‚   â”œâ”€â”€ schemas.py             # Pydantic è³‡æ–™æ¨¡å‹
â”‚   â””â”€â”€ requirements.txt       # å¾Œç«¯éœ€è¦çš„ Python å¥—ä»¶
â”œâ”€â”€ frontend/                 # Next.js 14ï¼ˆApp Routerï¼‰å‰ç«¯ä»‹é¢
â”‚   â”œâ”€â”€ app/                   # å„€è¡¨æ¿ã€ç—…æ‚£ã€ä¸Šå‚³ã€åˆ†æçµæœç­‰é é¢
â”‚   â”œâ”€â”€ components/            # å…±ç”¨å°è¦½åˆ—ã€å¡ç‰‡ã€å…ƒä»¶
â”‚   â”œâ”€â”€ lib/                   # å‹åˆ¥åŒ– API å‘¼å«å·¥å…·
â”‚   â””â”€â”€ package.json           # å‰ç«¯ä¾è³´èˆ‡è…³æœ¬
â”œâ”€â”€ docs/                     # ç³»çµ±è¨­è¨ˆæ–‡ä»¶
â”œâ”€â”€ models/                   # æ¨¡å‹æª”æ¡ˆæˆ–æª¢æŸ¥é»
â”œâ”€â”€ src/                      # å‚³çµ± PyTorch è¨“ç·´èˆ‡æ¨è«–è…³æœ¬
â””â”€â”€ data/                     # ç¯„ä¾‹è³‡æ–™é›†èˆ‡ç›¸é—œæª”æ¡ˆ
```

### 3. äº‹å‰æº–å‚™

| çµ„ä»¶ | éœ€æ±‚                                  |
| ---- | ------------------------------------- |
| å¾Œç«¯ | Python 3.10 ä»¥ä¸Šèˆ‡ `pip`              |
| å‰ç«¯ | Node.js 18 ä»¥ä¸Šèˆ‡ `npm` æˆ– `yarn`     |
| é¸ç”¨ | `make` æˆ–çµ‚ç«¯æ©Ÿç’°å¢ƒï¼Œç”¨æ–¼åŸ·è¡Œæ•´åˆè…³æœ¬ |

> **å°æç¤ºï¼š** å»ºè­°ä½¿ç”¨ Python è™›æ“¬ç’°å¢ƒéš”é›¢å¥—ä»¶ï¼Œé¿å…èˆ‡å…¶ä»–å°ˆæ¡ˆè¡çªã€‚

### 4. å¿«é€Ÿé–‹å§‹

1. **å–å¾—å°ˆæ¡ˆåŸå§‹ç¢¼**ï¼ˆæˆ–ç›´æ¥åœ¨ç¾æœ‰è³‡æ–™å¤¾é–‹å•Ÿï¼‰ã€‚
2. **å»ºç«‹ä¸¦å•Ÿå‹• Python è™›æ“¬ç’°å¢ƒï¼š**
   ```bash
   python -m venv .venv
   source .venv/bin/activate  # Windows: .venv\Scripts\activate
   ```
3. **å®‰è£å¾Œç«¯å¥—ä»¶ä¸¦å•Ÿå‹• APIï¼š**
   ```bash
   pip install -r backend/requirements.txt
   uvicorn backend.main:app --reload
   ```
   API é è¨­ä½¿ç”¨ `http://localhost:8000`ï¼Œä¸¦å›å‚³æ¨¡æ“¬è³‡æ–™ï¼Œæ–¹ä¾¿ç«‹å³æ¸¬è©¦ã€‚
4. **å®‰è£å‰ç«¯å¥—ä»¶ä¸¦å•Ÿå‹•é–‹ç™¼ä¼ºæœå™¨ï¼š**
   ```bash
   cd frontend
   npm install
   npm run dev
   ```
   å‰å¾€ç€è¦½å™¨é–‹å•Ÿ `http://localhost:3000`ï¼Œè«‹ç¢ºä¿å¾Œç«¯åœ¨å¦ä¸€å€‹çµ‚ç«¯æ©ŸæŒçºŒé‹è¡Œã€‚

### 5. å‰ç«¯ç’°å¢ƒè®Šæ•¸

è‹¥å¾Œç«¯æœå‹™ä¸åœ¨æœ¬æ©Ÿé è¨­ä½ç½®ï¼Œå¯è¨­å®š `NEXT_PUBLIC_API_BASE_URL` æŒ‡å‘æŒ‡å®šç¶²å€ï¼š

```bash
NEXT_PUBLIC_API_BASE_URL="https://your-api.example.com" npm run dev
```

### 6. æ“ä½œæµç¨‹ç¤ºä¾‹

1. é€²å…¥å„€è¡¨æ¿ï¼ˆç›®å‰ç‚ºæ¨¡æ“¬ç’°å¢ƒï¼Œä¸éœ€å¸³è™Ÿå¯†ç¢¼ï¼‰ã€‚
2. åœ¨ **Dashboard** æª¢è¦–ç³»çµ±çµ±è¨ˆã€åˆ†æéšŠåˆ—èˆ‡å¿«æ·æ“ä½œã€‚
3. é–‹å•Ÿ **Patients** é é¢ç€è¦½æˆ–æœå°‹ç—…æ‚£åå–®ï¼Œé»é¸ç—…æ‚£å¯æŸ¥çœ‹è©³ç´°è³‡æ–™èˆ‡æ­·å²ç´€éŒ„ã€‚
4. å‰å¾€ **Upload** é é¢æ‹–æ›³ä¸Šå‚³å£è…” X å…‰å½±åƒï¼ˆå¯ä½¿ç”¨ç¯„ä¾‹æª”æ¡ˆï¼‰ï¼Œå¡«å¯«ç—…æ‚£èˆ‡æª¢æŸ¥è³‡è¨Šå¾Œé€å‡ºï¼Œè§¸ç™¼æ¨¡æ“¬æµç¨‹ã€‚
5. åœ¨ **Analysis Result** é é¢æª¢è¦–ç³»çµ±ç”¢ç”Ÿçš„å ±å‘Šï¼ŒåŒ…å«æª¢æ¸¬çµæœã€AI åˆ†ææ‘˜è¦èˆ‡è™•ç†æ™‚é–“è»¸ã€‚

### 7. API åƒè€ƒï¼ˆé–‹ç™¼æ¨¡å¼ï¼‰

| è·¯å¾‘                      | æ–¹æ³•       | èªªæ˜                             |
| ------------------------- | ---------- | -------------------------------- |
| `/api/dashboard/overview` | GET        | ç³»çµ±ç¸½è¦½ã€éšŠåˆ—ç‹€æ…‹ã€æœ€æ–°ç—…æ‚£è³‡æ–™ |
| `/api/patients`           | GET / POST | å–å¾—ç—…æ‚£åˆ—è¡¨æˆ–æ–°å¢ç—…æ‚£           |
| `/api/analyses/{id}`      | GET        | å–å¾—ç‰¹å®šåˆ†æçš„è©³ç´°çµæœ           |
| `/api/images`             | POST       | ä¸Šå‚³å½±åƒè³‡æ–™ä»¥é€²è¡Œåˆ†æ           |
| `/api/analyses`           | POST       | å»ºç«‹æ¨¡æ“¬åˆ†æå·¥ä½œ                 |

æ‰€æœ‰å›æ‡‰çš†ç‚ºè¨­è¨ˆæ–‡ä»¶å°æ‡‰çš„ç¯„ä¾‹è³‡æ–™ï¼Œå¯åœ¨ç„¡çœŸå¯¦å¾Œç«¯æ•´åˆçš„æƒ…æ³ä¸‹å®Œæˆå‰ç«¯é–‹ç™¼ã€‚

### 8. å‚³çµ±æ©Ÿå™¨å­¸ç¿’å·¥å…·

`src/` ç›®éŒ„ä¿ç•™åŸå§‹çš„ PyTorch è…³æœ¬ï¼Œé©åˆæŒçºŒé€²è¡Œæ¨¡å‹è¨“ç·´æˆ–è©•ä¼°çš„ç ”ç©¶äººå“¡ã€‚æ¯æ”¯è…³æœ¬çš†æä¾›è¨»è§£ï¼Œèªªæ˜æ‰€éœ€çš„ CSV/DICOM æ ¼å¼èˆ‡è¨­å®šæ–¹å¼ã€‚

### 9. æ“´å……æ–¹å‘èˆ‡å»ºè­°

- å°‡å…§å»ºå„²å­˜æ”¹ç‚ºçœŸå¯¦è³‡æ–™åº«èˆ‡ç‰©ä»¶å„²å­˜æœå‹™ã€‚
- åœ¨ä¸Šå‚³æµç¨‹ä¸­ä¸²æ¥æª”æ¡ˆä¸Šå‚³æœå‹™æˆ–ä½¿ç”¨é ç°½åç¶²å€ã€‚
- ä¾æ“šè¨­è¨ˆæ–‡ä»¶å°å…¥èº«åˆ†é©—è­‰èˆ‡æ¬Šé™ç®¡ç†ï¼ˆä¾‹å¦‚ OAuth2 + RBACï¼‰ã€‚
- å°‡ `models/` å…§çš„å¯¦éš›æ¨è«–çµæœæ•´åˆé€²åˆ†ææµç¨‹ã€‚

### 10. Cross-Attention Demo ç¤ºç¯„

![Demo ä»‹é¢](docs/images/demo-interface.png)
_Cross-attention demo ä»‹é¢å±•ç¤ºæ¨£æœ¬æ¡ˆä¾‹èˆ‡ä¸Šå‚³åŠŸèƒ½_

**ğŸ¬ Cross-Attention Demo å½±ç‰‡**: [è§€çœ‹ AI åˆ†æå¯¦éš›é‹ä½œ](https://youtu.be/ONw7fAoHVxE)

- **å¾Œç«¯**ï¼š`demo_backend/main.py` æä¾› `/demo/samples` èˆ‡ `/demo/infer` ç«¯é»ï¼Œå¯é€é `DEMO_YOLO_WEIGHTS`ã€`DEMO_CLASSIFIER_WEIGHTS` ç­‰ç’°å¢ƒè®Šæ•¸æŒ‡å®šæ¨¡å‹æ¬Šé‡ã€‚
- **å‰ç«¯**ï¼šåœ¨ Next.js ä»‹é¢ä¸­é–‹å•Ÿ `/demo` é é¢ï¼Œå³å¯ç€è¦½å…§å»ºæ¨£æœ¬æˆ–ä¸Šå‚³å½±åƒæŸ¥çœ‹æ¨è«–èˆ‡ Grad-CAM ç–Šå±¤ã€‚
- **éœæ…‹è³‡ç”¢**ï¼šç¤ºç¯„å½±åƒæ”¾åœ¨ `demo_backend/static/samples/`ï¼Œåªè¦æ”¾å…¥ PNG/JPG æª”å³å¯è‡ªå‹•å‡ºç¾åœ¨ Demo é é¢ã€‚
- **è¼¸å‡ºæª”æ¡ˆ**ï¼šæ¨è«–ç”¢ç‰©å¯«å…¥ `demo_backend/outputs/`ï¼Œä¸¦ç”± FastAPI é€é `/demo-outputs` æä¾›éœæ…‹ä¸‹è¼‰ã€‚
