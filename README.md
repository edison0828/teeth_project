# Oral X-Ray Demo Platform

## English

### Overview

This branch packages a streamlined proof-of-concept for the cross-attention dental X-ray demo. The FastAPI service powers the `/demo` inference endpoints, and the Next.js page delivers the interactive viewer with Grad-CAM overlays and per-tooth predictions. The repository is intentionally focused on the demo experience so new contributors can start experimenting quickly.

### Repository Layout

```
teeth_project/
â”œâ”€â”€ demo_backend/           # FastAPI service that powers /demo/samples and /demo/infer
â”‚   â”œâ”€â”€ _vendor/            # Bundled inference utilities (cross-attention + YOLO helpers)
â”‚   â”œâ”€â”€ main.py             # FastAPI entrypoint for the demo API
â”‚   â”œâ”€â”€ pipeline.py         # Cross-attention inference pipeline
â”‚   â”œâ”€â”€ samples.py          # Static sample metadata loader
â”‚   â””â”€â”€ requirements.txt    # Python dependencies for demo_backend
â”œâ”€â”€ frontend/               # Next.js 14 App Router project
â”‚   â”œâ”€â”€ app/demo/page.tsx   # Demo UI route
â”‚   â””â”€â”€ lib/                # Minimal API client, types, and media helpers
â”œâ”€â”€ models/                 # YOLO detector and classifier checkpoints used by the demo
â””â”€â”€ README.md               # This file (English & Chinese instructions)
```

### Prerequisites

- Python 3.10 or newer with `pip`
- Node.js 18 or newer with `npm` or `yarn`
- (Optional) CUDA-capable GPU for faster inference; CPU mode works but is slower

### Quick Start

1. **Prepare a Python environment**

   ```bash
   python -m venv .venv
   .\.venv\Scripts\activate  # macOS/Linux: source .venv/bin/activate
   pip install -r demo_backend/requirements.txt
   ```

2. **Launch the FastAPI demo service**

   ```bash
   uvicorn demo_backend.main:app --reload
   ```

   The API exposes:

   - `GET /demo/samples` â€“ list of bundled sample radiographs
   - `POST /demo/infer` â€“ run cross-attention inference on an uploaded image or sample ID
   - `GET /demo-outputs/*` â€“ static files generated by the pipeline

3. **Run the Next.js frontend** (in a new terminal)
   ```bash
   cd frontend
   npm install
   npm run dev
   ```
   Visit `http://localhost:3000/demo` to access the UI. The page automatically targets the local FastAPI service. If you host the API elsewhere, export `NEXT_PUBLIC_API_BASE_URL` before running `npm run dev`.

### Model Weights

1. Download the required checkpoints from the shared Google Drive folder (link : https://drive.google.com/drive/folders/1Bw3phDv8uao05iKZ3LOJ1xcqRYI8hzfB?usp=sharing).
2. Copy `cross_attn_fdi_camAlignA.pth` (cross-attention classifier) and `fdi_all seg.pt` (YOLO detector) into the project-level `models/` directory. Create the folder if it does not exist.
3. (Optional) Place any additional weights your experiments rely on, such as `yolo_caries.pt`, in the same directory.

If you choose different locations or filenames, update the relevant environment variables (`DEMO_YOLO_WEIGHTS`, `DEMO_CLASSIFIER_WEIGHTS`) before starting the backend.

### Backend Configuration

Environment variables can be used to customize the runtime without editing code:

| Variable                  | Description                                    | Default                               |
| ------------------------- | ---------------------------------------------- | ------------------------------------- |
| `DEMO_YOLO_WEIGHTS`       | Path to the YOLO detector checkpoint           | `models/fdi_all seg.pt`               |
| `DEMO_CLASSIFIER_WEIGHTS` | Path to the cross-attention classifier weights | `models/cross_attn_fdi_camAlignA.pth` |
| `DEMO_LAYERED_THRESHOLDS` | Optional JSON thresholds file                  | _(unset)_                             |
| `DEMO_OUTPUT_DIR`         | Directory for inference outputs                | `demo_backend/outputs`                |
| `DEMO_STATIC_DIR`         | Directory that hosts static samples            | `demo_backend/static`                 |
| `DEMO_DEVICE`             | `cuda` or `cpu` device preference              | `cuda`                                |
| `DEMO_AUTOLOAD`           | Autoload models at startup (`true` / `false`)  | `false`                               |

Sample assets live in `demo_backend/static/samples/`. Add more JPG/PNG files to surface them automatically in the modal selector.

### Frontend Notes

- `frontend/lib/api.ts` exposes the typed helpers (`fetchDemoSamples`, `submitDemoInference`) that wire the demo page to the backend.
- `frontend/lib/types.ts` contains the shared demo response contracts.
- Tailwind CSS classes are scoped to the demo page to keep styles isolated from the rest of the application shell.

### Typical Demo Flow

1. Start the backend (`uvicorn demo_backend.main:app --reload`).
2. Open `http://localhost:3000/demo` and pick a predefined sample or upload a custom radiograph.
3. Toggle Grad-CAM overlays, zoom/pan the panoramic view, and inspect per-tooth predictions.
4. Download the generated overlay image or CSV from the backend-served output links.

---

## ä¸­æ–‡

### æ¦‚è¿°

æ­¤åˆ†æ”¯å°ˆæ³¨æ–¼äº¤å‰æ³¨æ„åŠ›ç‰™ç§‘ X å…‰ Demoã€‚FastAPI æœå‹™æä¾› `/demo` æ¨è«– APIï¼ŒNext.js `/demo` å–®ä¸€è·¯ç”±å‘ˆç¾äº’å‹•è¦–è¦ºåŒ–èˆ‡ Grad-CAM ç–Šå±¤ï¼Œè®“ç¤ºç¯„æµç¨‹ç°¡æ½”æ˜“ä¸Šæ‰‹ã€‚

### å°ˆæ¡ˆçµæ§‹

```
teeth_project/
â”œâ”€â”€ demo_backend/           # FastAPI Demo æœå‹™ï¼ˆæä¾› /demo/samples åŠ /demo/inferï¼‰
â”‚   â”œâ”€â”€ _vendor/            # å…§å»ºä¹‹æ¨è«–å·¥å…·ï¼ˆYOLO + Cross-Attentionï¼‰
â”‚   â”œâ”€â”€ main.py             # Demo API é€²å…¥é»
â”‚   â”œâ”€â”€ pipeline.py         # äº¤å‰æ³¨æ„åŠ›æ¨è«–æµç¨‹
â”‚   â”œâ”€â”€ samples.py          # ç¯„ä¾‹å½±åƒèˆ‡æè¿°
â”‚   â””â”€â”€ requirements.txt    # Demo å¾Œç«¯æ‰€éœ€å¥—ä»¶
â”œâ”€â”€ frontend/               # Next.js 14ï¼ˆApp Routerï¼‰å‰ç«¯
â”‚   â”œâ”€â”€ app/demo/page.tsx   # Demo ä¸»ä»‹é¢è·¯ç”±
â”‚   â””â”€â”€ lib/                # Demo å°ˆç”¨ API å·¥å…·èˆ‡å‹åˆ¥
â”œâ”€â”€ models/                 # Demo ä½¿ç”¨çš„ YOLO èˆ‡åˆ†é¡å™¨æ¬Šé‡
â””â”€â”€ README.md               # æ­¤æ–‡ä»¶ï¼ˆå«ä¸­è‹±æ–‡èªªæ˜ï¼‰
```

### ç’°å¢ƒéœ€æ±‚

- Python 3.10 ä»¥ä¸Šï¼ˆå»ºè­°æ­é…è™›æ“¬ç’°å¢ƒï¼‰
- Node.js 18 ä»¥ä¸Šï¼ˆå« npm æˆ– yarnï¼‰
- ï¼ˆé¸ç”¨ï¼‰æ”¯æ´ CUDA çš„ GPUï¼Œå¯ç¸®çŸ­æ¨è«–æ™‚é–“ï¼›è‹¥ç„¡å‰‡è‡ªå‹•ä½¿ç”¨ CPU

### å¿«é€Ÿå•Ÿå‹•

1. **å»ºç«‹ Python ç’°å¢ƒä¸¦å®‰è£å¥—ä»¶**

   ```bash
   python -m venv .venv
   .\.venv\Scripts\activate  # macOS/Linux: source .venv/bin/activate
   pip install -r demo_backend/requirements.txt
   ```

2. **å•Ÿå‹• FastAPI Demo æœå‹™**

   ```bash
   uvicorn demo_backend.main:app --reload
   ```

   ä¸»è¦ç«¯é»å¦‚ä¸‹ï¼š

   - `GET /demo/samples`ï¼šå–å¾—å…§å»ºç¤ºç¯„å½±åƒæ¸…å–®
   - `POST /demo/infer`ï¼šä¸Šå‚³å½±åƒæˆ–æŒ‡å®š `sample_id` è§¸ç™¼æ¨è«–
   - `GET /demo-outputs/*`ï¼šè®€å–æ¨è«–è¼¸å‡ºçš„ç–Šåœ–èˆ‡ CSV

3. **å•Ÿå‹•å‰ç«¯**ï¼ˆå¦é–‹çµ‚ç«¯æ©Ÿï¼‰
   ```bash
   cd frontend
   npm install
   npm run dev
   ```
   ç€è¦½ `http://localhost:3000/demo` å³å¯ä½¿ç”¨ä»‹é¢ã€‚è‹¥å¾Œç«¯éƒ¨ç½²æ–¼å…¶ä»–ä½ç½®ï¼Œè«‹åœ¨ `npm run dev` å‰è¨­å®š `NEXT_PUBLIC_API_BASE_URL`ã€‚

### æ¨¡å‹æ¬Šé‡

1. å¾ Google é›²ç«¯ç¡¬ç¢Ÿè³‡æ–™å¤¾ä¸‹è¼‰æ‰€éœ€çš„æ¨¡å‹æ¬Šé‡(https://drive.google.com/drive/folders/1Bw3phDv8uao05iKZ3LOJ1xcqRYI8hzfB?usp=sharing)ã€‚
2. å°‡ `cross_attn_fdi_camAlignA.pth`ï¼ˆäº¤å‰æ³¨æ„åŠ›åˆ†é¡å™¨ï¼‰èˆ‡ `fdi_all seg.pt`ï¼ˆYOLO åµæ¸¬å™¨ï¼‰æ”¾å…¥å°ˆæ¡ˆæ ¹ç›®éŒ„ä¸‹çš„ `models/` è³‡æ–™å¤¾ï¼Œå¦‚è³‡æ–™å¤¾ä¸å­˜åœ¨å¯è‡ªè¡Œå»ºç«‹ã€‚
3. ï¼ˆé¸ç”¨ï¼‰æŠŠå¯¦é©—æœƒç”¨åˆ°çš„å…¶ä»–æ¬Šé‡æª”ï¼Œä¾‹å¦‚ `yolo_caries.pt`ï¼Œä¹Ÿæ”¾åœ¨åŒä¸€è³‡æ–™å¤¾ä¸­å‚™ç”¨ã€‚

è‹¥æ¬Šé‡æª”æ”¾åœ¨å…¶ä»–è·¯å¾‘æˆ–ä½¿ç”¨è‡ªè¨‚æª”åï¼Œè«‹åœ¨å•Ÿå‹•å¾Œç«¯å‰èª¿æ•´å°æ‡‰çš„ç’°å¢ƒè®Šæ•¸ï¼ˆ`DEMO_YOLO_WEIGHTS`ã€`DEMO_CLASSIFIER_WEIGHTS`ï¼‰ã€‚

### å¾Œç«¯ç’°å¢ƒè®Šæ•¸

| è®Šæ•¸                      | èªªæ˜                                    | é è¨­å€¼                                |
| ------------------------- | --------------------------------------- | ------------------------------------- |
| `DEMO_YOLO_WEIGHTS`       | YOLO åˆ†å‰²æ¨¡å‹æ¬Šé‡è·¯å¾‘                   | `models/fdi_all seg.pt`               |
| `DEMO_CLASSIFIER_WEIGHTS` | äº¤å‰æ³¨æ„åŠ›åˆ†é¡å™¨æ¬Šé‡                    | `models/cross_attn_fdi_camAlignA.pth` |
| `DEMO_LAYERED_THRESHOLDS` | ï¼ˆé¸ç”¨ï¼‰å¤šå±¤é–€æª»è¨­å®š JSON               | _(æœªè¨­å®š)_                            |
| `DEMO_OUTPUT_DIR`         | æ¨è«–è¼¸å‡ºè³‡æ–™å¤¾                          | `demo_backend/outputs`                |
| `DEMO_STATIC_DIR`         | éœæ…‹è³‡ç”¢æ ¹ç›®éŒ„                          | `demo_backend/static`                 |
| `DEMO_DEVICE`             | æ¨è«–è£ç½®ï¼ˆ`cuda` æˆ– `cpu`ï¼‰             | `cuda`                                |
| `DEMO_AUTOLOAD`           | å•Ÿå‹•æ™‚æ˜¯å¦é å…ˆè¼‰å…¥æ¨¡å‹ (`true`/`false`) | `false`                               |

ç¤ºç¯„å½±åƒæ”¾åœ¨ `demo_backend/static/samples/`ã€‚æ–°å¢ JPG/PNG æª”å³å¯è‡ªå‹•å‡ºç¾åœ¨å‰ç«¯é¸å–®å…§ã€‚

### å‰ç«¯èªªæ˜

- `frontend/lib/api.ts` æä¾› `fetchDemoSamples` èˆ‡ `submitDemoInference` å…©å€‹è¼”åŠ©å‡½å¼ï¼Œç”¨æ–¼å–å¾—æ¨£æœ¬æ¸…å–®ä¸¦é€å‡ºæ¨è«–ã€‚
- `frontend/lib/types.ts` å®šç¾©æ¨è«–çµæœèˆ‡ç‰™é½’æª¢æ¸¬çš„å‹åˆ¥ã€‚
- Tailwind æ¨£å¼é›†ä¸­æ–¼ `/demo` é é¢ï¼Œä¿æŒå…¶ä»–åŠŸèƒ½å€çš„é¢¨æ ¼ç¨ç«‹ã€‚

### ç¤ºç¯„æµç¨‹

1. åŸ·è¡Œ `uvicorn demo_backend.main:app --reload` å•Ÿå‹•å¾Œç«¯ã€‚
2. æ‰“é–‹ `http://localhost:3000/demo`ï¼Œé¸æ“‡å…§å»ºæ¨£æœ¬æˆ–è‡ªè¡Œä¸Šå‚³å½±åƒã€‚
3. ç€è¦½æ¨è«–çµæœã€å•Ÿç”¨ Grad-CAM ç–Šå±¤ã€æ”¾å¤§/ç¸®å°å½±åƒä¸¦æŸ¥è©¢ç‰¹å®š FDI ç‰™ä½ã€‚
4. é€éä»‹é¢ä¸Šçš„é€£çµä¸‹è¼‰ç–Šå±¤å½±åƒæˆ– CSV çµæœæª”æ¡ˆã€‚

Enjoy exploring the cross-attention dental X-ray demo! / ç¥æ‚¨å±•ç¤ºé †åˆ© ğŸ‰
