# Oral X-Ray Demo Platform

## English

### Overview

This branch packages a streamlined proof-of-concept for the cross-attention dental X-ray demo. The FastAPI service powers the `/demo` inference endpoints, and the Next.js page delivers the interactive viewer with Grad-CAM overlays and per-tooth predictions. The repository is intentionally focused on the demo experience so new contributors can start experimenting quickly.

### Repository Layout

```
teeth_project/
├── demo_backend/           # FastAPI service that powers /demo/samples and /demo/infer
│   ├── _vendor/            # Bundled inference utilities (cross-attention + YOLO helpers)
│   ├── main.py             # FastAPI entrypoint for the demo API
│   ├── pipeline.py         # Cross-attention inference pipeline
│   ├── samples.py          # Static sample metadata loader
│   └── requirements.txt    # Python dependencies for demo_backend
├── frontend/               # Next.js 14 App Router project
│   ├── app/demo/page.tsx   # Demo UI route
│   └── lib/                # Minimal API client, types, and media helpers
├── models/                 # YOLO detector and classifier checkpoints used by the demo
└── README.md               # This file (English & Chinese instructions)
```

### Prerequisites

- Python 3.10 or newer with `pip`
- Node.js 18 or newer with `npm` or `yarn`
- (Optional) CUDA-capable GPU for faster inference; CPU mode works but is slower

### Quick Start

1. **Prepare a Python environment**

   ```bash
   python -m venv .venv
   .\.venv\Scripts\activate  # macOS/Linux: source .venv/bin/activate
   pip install -r demo_backend/requirements.txt
   ```

2. **Launch the FastAPI demo service**

   ```bash
   uvicorn demo_backend.main:app --reload
   ```

   The API exposes:

   - `GET /demo/samples` – list of bundled sample radiographs
   - `POST /demo/infer` – run cross-attention inference on an uploaded image or sample ID
   - `GET /demo-outputs/*` – static files generated by the pipeline

3. **Run the Next.js frontend** (in a new terminal)
   ```bash
   cd frontend
   npm install
   npm run dev
   ```
   Visit `http://localhost:3000/demo` to access the UI. The page automatically targets the local FastAPI service. If you host the API elsewhere, export `NEXT_PUBLIC_API_BASE_URL` before running `npm run dev`.

### Model Weights

1. Download the required checkpoints from the shared Google Drive folder (link : https://drive.google.com/drive/folders/1Bw3phDv8uao05iKZ3LOJ1xcqRYI8hzfB?usp=sharing).
2. Copy `cross_attn_fdi_camAlignA.pth` (cross-attention classifier) and `fdi_all seg.pt` (YOLO detector) into the project-level `models/` directory. Create the folder if it does not exist.
3. (Optional) Place any additional weights your experiments rely on, such as `yolo_caries.pt`, in the same directory.

If you choose different locations or filenames, update the relevant environment variables (`DEMO_YOLO_WEIGHTS`, `DEMO_CLASSIFIER_WEIGHTS`) before starting the backend.

### Backend Configuration

Environment variables can be used to customize the runtime without editing code:

| Variable                  | Description                                    | Default                               |
| ------------------------- | ---------------------------------------------- | ------------------------------------- |
| `DEMO_YOLO_WEIGHTS`       | Path to the YOLO detector checkpoint           | `models/fdi_all seg.pt`               |
| `DEMO_CLASSIFIER_WEIGHTS` | Path to the cross-attention classifier weights | `models/cross_attn_fdi_camAlignA.pth` |
| `DEMO_LAYERED_THRESHOLDS` | Optional JSON thresholds file                  | _(unset)_                             |
| `DEMO_OUTPUT_DIR`         | Directory for inference outputs                | `demo_backend/outputs`                |
| `DEMO_STATIC_DIR`         | Directory that hosts static samples            | `demo_backend/static`                 |
| `DEMO_DEVICE`             | `cuda` or `cpu` device preference              | `cuda`                                |
| `DEMO_AUTOLOAD`           | Autoload models at startup (`true` / `false`)  | `false`                               |

Sample assets live in `demo_backend/static/samples/`. Add more JPG/PNG files to surface them automatically in the modal selector.

### Frontend Notes

- `frontend/lib/api.ts` exposes the typed helpers (`fetchDemoSamples`, `submitDemoInference`) that wire the demo page to the backend.
- `frontend/lib/types.ts` contains the shared demo response contracts.
- Tailwind CSS classes are scoped to the demo page to keep styles isolated from the rest of the application shell.

### Typical Demo Flow

1. Start the backend (`uvicorn demo_backend.main:app --reload`).
2. Open `http://localhost:3000/demo` and pick a predefined sample or upload a custom radiograph.
3. Toggle Grad-CAM overlays, zoom/pan the panoramic view, and inspect per-tooth predictions.
4. Download the generated overlay image or CSV from the backend-served output links.

---

## 中文

### 概述

此分支專注於交叉注意力牙科 X 光 Demo。FastAPI 服務提供 `/demo` 推論 API，Next.js `/demo` 單一路由呈現互動視覺化與 Grad-CAM 疊層，讓示範流程簡潔易上手。

### 專案結構

```
teeth_project/
├── demo_backend/           # FastAPI Demo 服務（提供 /demo/samples 及 /demo/infer）
│   ├── _vendor/            # 內建之推論工具（YOLO + Cross-Attention）
│   ├── main.py             # Demo API 進入點
│   ├── pipeline.py         # 交叉注意力推論流程
│   ├── samples.py          # 範例影像與描述
│   └── requirements.txt    # Demo 後端所需套件
├── frontend/               # Next.js 14（App Router）前端
│   ├── app/demo/page.tsx   # Demo 主介面路由
│   └── lib/                # Demo 專用 API 工具與型別
├── models/                 # Demo 使用的 YOLO 與分類器權重
└── README.md               # 此文件（含中英文說明）
```

### 環境需求

- Python 3.10 以上（建議搭配虛擬環境）
- Node.js 18 以上（含 npm 或 yarn）
- （選用）支援 CUDA 的 GPU，可縮短推論時間；若無則自動使用 CPU

### 快速啟動

1. **建立 Python 環境並安裝套件**

   ```bash
   python -m venv .venv
   .\.venv\Scripts\activate  # macOS/Linux: source .venv/bin/activate
   pip install -r demo_backend/requirements.txt
   ```

2. **啟動 FastAPI Demo 服務**

   ```bash
   uvicorn demo_backend.main:app --reload
   ```

   主要端點如下：

   - `GET /demo/samples`：取得內建示範影像清單
   - `POST /demo/infer`：上傳影像或指定 `sample_id` 觸發推論
   - `GET /demo-outputs/*`：讀取推論輸出的疊圖與 CSV

3. **啟動前端**（另開終端機）
   ```bash
   cd frontend
   npm install
   npm run dev
   ```
   瀏覽 `http://localhost:3000/demo` 即可使用介面。若後端部署於其他位置，請在 `npm run dev` 前設定 `NEXT_PUBLIC_API_BASE_URL`。

### 模型權重

1. 從 Google 雲端硬碟資料夾下載所需的模型權重(https://drive.google.com/drive/folders/1Bw3phDv8uao05iKZ3LOJ1xcqRYI8hzfB?usp=sharing)。
2. 將 `cross_attn_fdi_camAlignA.pth`（交叉注意力分類器）與 `fdi_all seg.pt`（YOLO 偵測器）放入專案根目錄下的 `models/` 資料夾，如資料夾不存在可自行建立。
3. （選用）把實驗會用到的其他權重檔，例如 `yolo_caries.pt`，也放在同一資料夾中備用。

若權重檔放在其他路徑或使用自訂檔名，請在啟動後端前調整對應的環境變數（`DEMO_YOLO_WEIGHTS`、`DEMO_CLASSIFIER_WEIGHTS`）。

### 後端環境變數

| 變數                      | 說明                                    | 預設值                                |
| ------------------------- | --------------------------------------- | ------------------------------------- |
| `DEMO_YOLO_WEIGHTS`       | YOLO 分割模型權重路徑                   | `models/fdi_all seg.pt`               |
| `DEMO_CLASSIFIER_WEIGHTS` | 交叉注意力分類器權重                    | `models/cross_attn_fdi_camAlignA.pth` |
| `DEMO_LAYERED_THRESHOLDS` | （選用）多層門檻設定 JSON               | _(未設定)_                            |
| `DEMO_OUTPUT_DIR`         | 推論輸出資料夾                          | `demo_backend/outputs`                |
| `DEMO_STATIC_DIR`         | 靜態資產根目錄                          | `demo_backend/static`                 |
| `DEMO_DEVICE`             | 推論裝置（`cuda` 或 `cpu`）             | `cuda`                                |
| `DEMO_AUTOLOAD`           | 啟動時是否預先載入模型 (`true`/`false`) | `false`                               |

示範影像放在 `demo_backend/static/samples/`。新增 JPG/PNG 檔即可自動出現在前端選單內。

### 前端說明

- `frontend/lib/api.ts` 提供 `fetchDemoSamples` 與 `submitDemoInference` 兩個輔助函式，用於取得樣本清單並送出推論。
- `frontend/lib/types.ts` 定義推論結果與牙齒檢測的型別。
- Tailwind 樣式集中於 `/demo` 頁面，保持其他功能區的風格獨立。

### 示範流程

1. 執行 `uvicorn demo_backend.main:app --reload` 啟動後端。
2. 打開 `http://localhost:3000/demo`，選擇內建樣本或自行上傳影像。
3. 瀏覽推論結果、啟用 Grad-CAM 疊層、放大/縮小影像並查詢特定 FDI 牙位。
<!-- 4. 透過介面上的連結下載疊層影像或 CSV 結果檔案。 -->
